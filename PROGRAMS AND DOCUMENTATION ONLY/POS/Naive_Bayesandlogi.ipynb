{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eYOwUAMpIPuU"
   },
   "source": [
    "\n",
    "#Author: Srikaran Elakurthy\n",
    "\n",
    "Import the POS oversampled train set and test set for using in Naive Bayes model and logistic model\n",
    "\n",
    "Implementing the Multinomial Naive bayes classification on the above dataset.\n",
    "Performing Cross validation with kfolds =5 and extract average accuracy\n",
    "Performing the naive bayes model on whole train data and test it and storing the results into files\n",
    "\n",
    "Implementing the logistic multi class classification on the above dataset.\n",
    "Performing Cross validation with kfolds =5 and extract average accuracy and predicted the test dataset and stored the reults.\n",
    "\n",
    "*Detailed description is written for many parts of the code below. Please read through for the same.\n",
    "\n",
    "#Command to run the file: \n",
    "\n",
    ">Open the ipynb notebook in Jupyter Lab and go to the menu bar on the top, click on 'Run' and from the dropdown select the 'Run All' option to run\n",
    "all the cells in the notebook. \n",
    "\n",
    "#Inputs and Outputs\n",
    "\n",
    "Inputs:\n",
    "> posovrsamprockredtrain.csv - The new data set with POS tags obtained from POS pre-processing script. This is only the train set.\n",
    "\n",
    "> posovrsamprockredtest.csv - The new data set with POS tags obtained from POS pre-processing script. This is only the test set.\n",
    "\n",
    "\n",
    "\n",
    "Outputs:\n",
    "> Naiveposrockred1.pkl-Naiveposrockred2.pkl-Naiveposrockred3.pkl-Naiveposrockred4.pkl-Naiveposrockred5.pkl <- Cross validation naive bayes models\n",
    "> respos_naiverockred1.csv-respos_naiverockred2.csv-respos_naiverockred3.csv-respos_naiverockred4.csv-respos_naiverockred5.csv <- cross validation results of naives bayes consists of csv files with actual and predicted values.\n",
    "> Naivepos_results_rockred.txt <- Cross vsalidation  classification reports.\n",
    "\n",
    ">logipos_rockred1.pkl , logipos_rockred2.pkl , logipos_rockred3.pkl ,logipos_rockred4.pkl ,logipos_rockred5.pkl <- Cross validation Logistic models\n",
    "\n",
    "> respos_logi_pos_rockred1.csv , respos_logi_pos_rockred2.csv , respos_logi_pos_rockred3.csv,respos_logi_pos_rockred4.csv, respos_logi_pos_rockred5.csv <-  cross validation results of logistic consists of csv files with actual and predicted values.\n",
    "\n",
    ">logipos_results.txt<- Cross vsalidation  classification reports.\n",
    "\n",
    ">Naiveposrockred_full.pkl <-  final model for naive bayes\n",
    "\n",
    ">respos_naiverockred_full.csv<- test data predicted values and actual values are presented in a csv file.\n",
    "\n",
    ">Naivepos_results_full.txt<- Cross validation report of final naive bayes model.\n",
    "\n",
    ">logiposrockred_full.pkl<- final model for logistic model \n",
    "\n",
    ">respos_logirockred_full.csv <- test data logistic predicted values and actual values are presented in a csv file\n",
    "\n",
    ">logipos_results_full.txt <- Classification report for a logistic model\n",
    "\n",
    "\n",
    "\n",
    "INPUT: The inputs are posovrsamprockredtrain.csv and posovrsamprockredtest.csv\n",
    "OUTPUT:\n",
    "\n",
    "For Naive BAyes:\n",
    "report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       Other       0.85      0.02      0.03      5753\n",
    "        Rock       0.98      0.06      0.12      5725\n",
    "     Country       0.96      0.75      0.84      5694\n",
    "     Electronic    0.80      0.66      0.72      5708\n",
    "       Indie       1.00      0.13      0.24      5448\n",
    "       Metal       0.66      0.05      0.09      4340\n",
    "     Hip-Hop       0.97      0.16      0.27      5650\n",
    "         Pop       0.72      0.20      0.31      5433\n",
    "        Folk       0.52      0.03      0.06      7014\n",
    "         R&B       0.94      0.20      0.33      5381\n",
    "        Jazz       0.19      0.99      0.32     10111\n",
    "    accuracy                           0.34     66257\n",
    "    macro avg       0.78      0.30      0.30     66257\n",
    "    weighted avg       0.74      0.34      0.30     66257\n",
    "\n",
    "For Logistic model:\n",
    "report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       Other       0.87      0.09      0.16      5753\n",
    "        Rock       0.51      0.91      0.65      5725\n",
    "     Country       0.82      0.99      0.90      5694\n",
    "     Electronic    0.93      0.61      0.73      5708\n",
    "       Indie       0.97      0.67      0.79      5448\n",
    "       Metal       0.47      0.75      0.57      4340\n",
    "     Hip-Hop       0.66      0.78      0.71      5650\n",
    "         Pop       0.32      0.96      0.49      5433\n",
    "        Folk       0.57      0.16      0.25      7014\n",
    "         R&B       0.66      0.96      0.78      5381\n",
    "        Jazz       0.67      0.10      0.18     10111\n",
    "\n",
    "    accuracy                           0.58     66257\n",
    "    macro avg       0.68      0.63      0.57     66257\n",
    "    weighted avg       0.68      0.58      0.53     66257\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dBtn_bd3I9F-"
   },
   "source": [
    "Importing the necessary python packages and reading pos over sampled train data and removing the unnamed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "utfs8_N0kyJ-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#from langdetect import detect\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "#import emoji\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import classification_report\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "df=pd.read_csv(\"posovrsamprockredtrain.csv\")\n",
    "df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "#df.drop('index', axis=1, inplace=True)\n",
    "#df=shuffle(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1589193985115,
     "user": {
      "displayName": "Srikaran Elakurthy",
      "photoUrl": "",
      "userId": "11196471557604689597"
     },
     "user_tz": 240
    },
    "id": "n1yYvpAOl4PS",
    "outputId": "d9a0784a-f244-44f6-d73d-63f70373a064"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>genre</th>\n",
       "      <th>DT</th>\n",
       "      <th>NN</th>\n",
       "      <th>JJ</th>\n",
       "      <th>IN</th>\n",
       "      <th>FW</th>\n",
       "      <th>VBG</th>\n",
       "      <th>VBD</th>\n",
       "      <th>VBN</th>\n",
       "      <th>TO</th>\n",
       "      <th>VB</th>\n",
       "      <th>RB</th>\n",
       "      <th>VBZ</th>\n",
       "      <th>.</th>\n",
       "      <th>NNS</th>\n",
       "      <th>VBP</th>\n",
       "      <th>WRB</th>\n",
       "      <th>PRP</th>\n",
       "      <th>PRP$</th>\n",
       "      <th>WP</th>\n",
       "      <th>CC</th>\n",
       "      <th>CD</th>\n",
       "      <th>MD</th>\n",
       "      <th>RP</th>\n",
       "      <th>WDT</th>\n",
       "      <th>UH</th>\n",
       "      <th>JJR</th>\n",
       "      <th>PDT</th>\n",
       "      <th>JJS</th>\n",
       "      <th>EX</th>\n",
       "      <th>RBR</th>\n",
       "      <th>RBS</th>\n",
       "      <th>NNP</th>\n",
       "      <th>WP$</th>\n",
       "      <th>$</th>\n",
       "      <th>NNPS</th>\n",
       "      <th>SYM</th>\n",
       "      <th>POS</th>\n",
       "      <th>LS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a kay a kay jordan de shoe pairan vich kaali h...</td>\n",
       "      <td>Other</td>\n",
       "      <td>3.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i remeb when i saw you at the diner s yet my h...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you left the water run when you left me here b...</td>\n",
       "      <td>Country</td>\n",
       "      <td>21.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thing that happen when you fall asleep when yo...</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oh my god my heart s as heavi as a stone and w...</td>\n",
       "      <td>Indie</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics       genre  ...  POS   LS\n",
       "0  a kay a kay jordan de shoe pairan vich kaali h...       Other  ...  0.0  0.0\n",
       "1  i remeb when i saw you at the diner s yet my h...        Rock  ...  0.0  0.0\n",
       "2  you left the water run when you left me here b...     Country  ...  0.0  0.0\n",
       "3  thing that happen when you fall asleep when yo...  Electronic  ...  0.0  0.0\n",
       "4  oh my god my heart s as heavi as a stone and w...       Indie  ...  0.0  0.0\n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sDp7yseXJPfq"
   },
   "source": [
    "Dividing the dataframe into lyrics and genre and pos data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sq0gM94LlsCC"
   },
   "outputs": [],
   "source": [
    "rawdf=df.iloc[:,0:2]\n",
    "posdf=df.iloc[:,2:41]\n",
    "lyrics=rawdf['lyrics']\n",
    "genre=rawdf['genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 439,
     "status": "ok",
     "timestamp": 1589194002549,
     "user": {
      "displayName": "Srikaran Elakurthy",
      "photoUrl": "",
      "userId": "11196471557604689597"
     },
     "user_tz": 240
    },
    "id": "E5T67pC1mvqU",
    "outputId": "b6d0f2d5-4f25-4c35-bee0-f09cad56625e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    a kay a kay jordan de shoe pairan vich kaali h...\n",
       "1    i remeb when i saw you at the diner s yet my h...\n",
       "2    you left the water run when you left me here b...\n",
       "3    thing that happen when you fall asleep when yo...\n",
       "4    oh my god my heart s as heavi as a stone and w...\n",
       "Name: lyrics, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 953,
     "status": "ok",
     "timestamp": 1589194005845,
     "user": {
      "displayName": "Srikaran Elakurthy",
      "photoUrl": "",
      "userId": "11196471557604689597"
     },
     "user_tz": 240
    },
    "id": "bRx85XwTmyCG",
    "outputId": "9f4793c9-f0a7-4704-c443-539b038305d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Other\n",
       "1          Rock\n",
       "2       Country\n",
       "3    Electronic\n",
       "4         Indie\n",
       "Name: genre, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lHdTZnuFJZZ6"
   },
   "source": [
    "Performing cross validation with 5 splits and implementing tfidf vectorization inside the cross validation to avoid any data leakage.\n",
    "TFidf Vectorization will consider parameters stop_words= english specifying that it will be removing any english words and says to consider both unigrams and bigrams.\n",
    "\n",
    "  We will extract the pos data using the train and test indexes and perform normalisation on pos data and horizontally stack withtfidf matrix and as given as a input to model.\n",
    "\n",
    "We are taking Multinomial Naive bayes model specifying we are seeking a multi class problem and alpha =1 specifying smoothing is allowed. \n",
    "Storing the models in pickle files using joblib and results with actual and predicted values into a csv file.\n",
    "Storing the accuracy of every cross validation split into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 420202,
     "status": "ok",
     "timestamp": 1589148541441,
     "user": {
      "displayName": "Srikaran Elakurthy",
      "photoUrl": "",
      "userId": "11196471557604689597"
     },
     "user_tz": 240
    },
    "id": "zNPkXEJnkyKF",
    "outputId": "f70b42fe-524e-44d7-d548-5cfab5b6c77b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 51850  51866  51875 ... 265023 265024 265025]\n",
      "[    0     1     2 ... 54345 54349 54374]\n",
      "tfidf1\n",
      "Normalizer1\n",
      "0.31121005169226124\n",
      "[     0      1      2 ... 265023 265024 265025]\n",
      "[ 51850  51866  51875 ... 107644 107658 107681]\n",
      "tfidf2\n",
      "Normalizer2\n",
      "0.30770682011131023\n",
      "[     0      1      2 ... 265023 265024 265025]\n",
      "[104764 104787 104788 ... 160528 160554 160556]\n",
      "tfidf3\n",
      "Normalizer3\n",
      "0.31055560796151305\n",
      "[     0      1      2 ... 265023 265024 265025]\n",
      "[158303 158310 158313 ... 213057 213074 213081]\n",
      "tfidf4\n",
      "Normalizer4\n",
      "0.30865012734647673\n",
      "[     0      1      2 ... 213057 213074 213081]\n",
      "[211342 211344 211365 ... 265023 265024 265025]\n",
      "tfidf5\n",
      "Normalizer5\n",
      "0.3131591359305726\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=0)\n",
    "i=0\n",
    "naiverep=[]\n",
    "naivescore=[]\n",
    "for train_index, test_index in skf.split(lyrics, genre):\n",
    "    print(train_index)\n",
    "    print(test_index)\n",
    "    x_train1, x_test1 = lyrics[train_index], lyrics[test_index]\n",
    "    y_train, y_test = genre[train_index], genre[test_index]\n",
    "    i=i+1\n",
    "    tfidf = TfidfVectorizer(stop_words=\"english\",ngram_range=(1,2))\n",
    "    x_train1 = tfidf.fit_transform(x_train1)\n",
    "    x_test1 = tfidf.transform(x_test1)\n",
    "    print(\"tfidf\"+str(i))\n",
    "    posx_train=posdf.iloc[train_index]\n",
    "    posx_test=posdf.iloc[test_index]\n",
    "    scaler = Normalizer().fit(posx_train)\n",
    "    normalizedX_tr = scaler.transform(posx_train)\n",
    "    normalizedX_ts = scaler.transform(posx_test)\n",
    "    x_train=hstack( [x_train1,normalizedX_tr] )\n",
    "    x_test=hstack( [x_test1,normalizedX_ts] )\n",
    "    print(\"Normalizer\"+str(i))\n",
    "    clf = MultinomialNB(alpha=1.0)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    joblib.dump(clf, 'Naiveposrockred'+str(i)+'.pkl')\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    rep=classification_report(y_test, y_pred, target_names=df.genre.unique())\n",
    "    naiverep.append(naiverep)\n",
    "    dat={'Actual':y_test,'pred':y_pred}\n",
    "    resdf=pd.DataFrame(dat)\n",
    "    resdf.to_csv('respos_naiverockred'+str(i)+'.csv')\n",
    "    naivescore.append(score)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvZATqsWJ9S4"
   },
   "source": [
    "Creating a file to write our classification report for our 5 results of Cross validation models and computing the average accuracy and writing them into the file.\n",
    "The classification report is computed by using the results csv files containing the actual and predicted values of ech split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1CC0OB0tphgS"
   },
   "outputs": [],
   "source": [
    "f=open(\"Naivepos_results_rockred.txt\",\"a\")\n",
    "for i in range(1,6):\n",
    "    f.write(\"\\n report\"+str(i)+\":\\n\")\n",
    "    dfr=pd.read_csv(\"respos_naiverockred\"+str(i)+\".csv\")\n",
    "    dfr.drop(dfr.columns[dfr.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "    f.write(classification_report(dfr.Actual, dfr.pred, target_names=df.genre.unique()))\n",
    "f.write(\"\\nThe average score for this model is\")\n",
    "f.write(str(mean(naivescore)))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D9hNtcs1KG7_"
   },
   "source": [
    "Performing cross validation with 5 splits and implementing tfidf vectorization inside the cross validation to avoid any data leakage.\n",
    "\n",
    "\n",
    "\n",
    "*   TFidf Vectorization will consider parameters stop_words= english specifying that it will be removing any english words and says to consider both unigrams and bigrams.\n",
    "*   We will extract the pos data using the train and test indexes and perform normalisation on pos data and horizontally stack withtfidf matrix and as given as a input to model.\n",
    "\n",
    "*   We are specifying the parameters on logistic model as multiclass as multinonial specifying we are seeking a multi class problem and optimization algorithm as 'sag'. Using sag so that the it is best for converging fastly on large datasets.\n",
    "\n",
    "\n",
    "*   Storing the models in pickle files using joblib  and results with actual and predicted values into a csv file.\n",
    "*   Storing the accuracy of every cross validation split into a list.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 942438,
     "status": "ok",
     "timestamp": 1589150214667,
     "user": {
      "displayName": "Srikaran Elakurthy",
      "photoUrl": "",
      "userId": "11196471557604689597"
     },
     "user_tz": 240
    },
    "id": "KM84iiRqkyKN",
    "outputId": "45d796f4-1b44-4501-c307-eb525e028990"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 51850  51866  51875 ... 265023 265024 265025]\n",
      "[    0     1     2 ... 54345 54349 54374]\n",
      "tfidf1\n",
      "Normalizer1\n",
      "0.7411613779572124\n",
      "[     0      1      2 ... 265023 265024 265025]\n",
      "[ 51850  51866  51875 ... 107644 107658 107681]\n",
      "tfidf2\n",
      "Normalizer2\n",
      "0.7391755494764645\n",
      "[     0      1      2 ... 265023 265024 265025]\n",
      "[104764 104787 104788 ... 160528 160554 160556]\n",
      "tfidf3\n",
      "Normalizer3\n",
      "0.7411942269597208\n",
      "[     0      1      2 ... 265023 265024 265025]\n",
      "[158303 158310 158313 ... 213057 213074 213081]\n",
      "tfidf4\n",
      "Normalizer4\n",
      "0.7417602113008207\n",
      "[     0      1      2 ... 213057 213074 213081]\n",
      "[211342 211344 211365 ... 265023 265024 265025]\n",
      "tfidf5\n",
      "Normalizer5\n",
      "0.741665880577304\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=0)\n",
    "i=0\n",
    "logirep=[]\n",
    "logiscore=[]\n",
    "for train_index, test_index in skf.split(lyrics, genre):\n",
    "    print(train_index)\n",
    "    print(test_index)\n",
    "    x_train1, x_test1 = lyrics[train_index], lyrics[test_index]\n",
    "    y_train, y_test = genre[train_index], genre[test_index]\n",
    "    i=i+1\n",
    "    tfidf = TfidfVectorizer(stop_words=\"english\",ngram_range=(1,2))\n",
    "    x_train1 = tfidf.fit_transform(x_train1)\n",
    "    x_test1 = tfidf.transform(x_test1)\n",
    "    print(\"tfidf\"+str(i))\n",
    "    posx_train=posdf.iloc[train_index]\n",
    "    posx_test=posdf.iloc[test_index]\n",
    "    scaler = Normalizer().fit(posx_train)\n",
    "    normalizedX_tr = scaler.transform(posx_train)\n",
    "    normalizedX_ts = scaler.transform(posx_test)\n",
    "    x_train=hstack( [x_train1,normalizedX_tr] )\n",
    "    x_test=hstack( [x_test1,normalizedX_ts] )\n",
    "    print(\"Normalizer\"+str(i))\n",
    "    clf = LogisticRegression(multi_class='multinomial',solver='sag')\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    joblib.dump(clf, 'logipos_rockred'+str(i)+'.pkl')\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    rep=classification_report(y_test, y_pred, target_names=df.genre.unique())\n",
    "    logirep.append(naiverep)\n",
    "    dat={'Actual':y_test,'pred':y_pred}\n",
    "    resdf=pd.DataFrame(dat)\n",
    "    resdf.to_csv('respos_logi_pos_rockred'+str(i)+'.csv')\n",
    "    logiscore.append(score)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wz7Rs0j8KcL2"
   },
   "source": [
    "Perform tfidf vectorization to extraxt tfidf matrix on whole training data and use the fitted vectorizer to transform the test data to tfidf matrix.\n",
    "\n",
    "Tfidf matrix considers:\n",
    "*   Removing stop words\n",
    "*   Considering both unigrams and bigrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZm_1Bn0q4r3"
   },
   "outputs": [],
   "source": [
    "f=open(\"logipos_results.txt\",\"a\")\n",
    "\n",
    "for i in range(1,6):\n",
    "    f.write(\"\\n report\"+str(i)+\":\\n\")\n",
    "    dfr=pd.read_csv(\"respos_logi_pos_rockred\"+str(i)+\".csv\")\n",
    "    dfr.drop(dfr.columns[dfr.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "    f.write(classification_report(dfr.Actual, dfr.pred, target_names=df.genre.unique()))\n",
    "f.write(\"\\nThe average score for this model is\")\n",
    "f.write(str(mean(logiscore)))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2_NradWAKvmN"
   },
   "source": [
    "Importing the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eQCgw1ATvdfS"
   },
   "outputs": [],
   "source": [
    "tdf=pd.read_csv(\"posovrsamprockredtest.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dSX3ula0KydD"
   },
   "source": [
    "Removing the unnamed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yox9eygXv-n4"
   },
   "outputs": [],
   "source": [
    "tdf.drop(tdf.columns[tdf.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 802
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 883,
     "status": "ok",
     "timestamp": 1589194027147,
     "user": {
      "displayName": "Srikaran Elakurthy",
      "photoUrl": "",
      "userId": "11196471557604689597"
     },
     "user_tz": 240
    },
    "id": "tzv1gxcOv6H1",
    "outputId": "e3dc950d-2f17-4937-f915-0e2fdabc4d97"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>genre</th>\n",
       "      <th>WRB</th>\n",
       "      <th>DT</th>\n",
       "      <th>NN</th>\n",
       "      <th>VB</th>\n",
       "      <th>TO</th>\n",
       "      <th>IN</th>\n",
       "      <th>PRP$</th>\n",
       "      <th>JJ</th>\n",
       "      <th>WDT</th>\n",
       "      <th>VBZ</th>\n",
       "      <th>RB</th>\n",
       "      <th>VBG</th>\n",
       "      <th>PRP</th>\n",
       "      <th>MD</th>\n",
       "      <th>VBP</th>\n",
       "      <th>RP</th>\n",
       "      <th>CC</th>\n",
       "      <th>VBD</th>\n",
       "      <th>JJS</th>\n",
       "      <th>CD</th>\n",
       "      <th>VBN</th>\n",
       "      <th>EX</th>\n",
       "      <th>JJR</th>\n",
       "      <th>NNS</th>\n",
       "      <th>PDT</th>\n",
       "      <th>WP</th>\n",
       "      <th>.</th>\n",
       "      <th>UH</th>\n",
       "      <th>FW</th>\n",
       "      <th>RBR</th>\n",
       "      <th>RBS</th>\n",
       "      <th>WP$</th>\n",
       "      <th>NNP</th>\n",
       "      <th>''</th>\n",
       "      <th>POS</th>\n",
       "      <th>SYM</th>\n",
       "      <th>$</th>\n",
       "      <th>NNPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when a cold chill begin to burn at your veri s...</td>\n",
       "      <td>R&amp;B</td>\n",
       "      <td>8.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when the boy wa no more than a shaver hi old m...</td>\n",
       "      <td>Country</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mayb i can t live to love you as long as i wan...</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>artist erick sermon f al green album react son...</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>9.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in a simpl life dream die hard you never let e...</td>\n",
       "      <td>Country</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics    genre  ...    $  NNPS\n",
       "0  when a cold chill begin to burn at your veri s...      R&B  ...  0.0   0.0\n",
       "1  when the boy wa no more than a shaver hi old m...  Country  ...  0.0   0.0\n",
       "2  mayb i can t live to love you as long as i wan...     Jazz  ...  0.0   0.0\n",
       "3  artist erick sermon f al green album react son...  Hip-Hop  ...  0.0   0.0\n",
       "4  in a simpl life dream die hard you never let e...  Country  ...  0.0   0.0\n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FAXfHlY_K20_"
   },
   "source": [
    "Dividing the dataframe into lyrics and genre and pos data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WrYsxnZjvv0U"
   },
   "outputs": [],
   "source": [
    "trawdf=tdf.iloc[:,0:2]\n",
    "tposdf=tdf.iloc[:,2:41]\n",
    "tlyrics=trawdf['lyrics']\n",
    "tgenre=trawdf['genre']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ukIt9c_LAPj"
   },
   "source": [
    "TFidf Vectorization will consider parameters stop_words= english specifying that it will be removing any english words and says to consider both unigrams and bigrams.\n",
    "*   We will use the pos data to perform normalisation on pos data and horizontally stack with tfidf matrix and as given as a input to model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 96048,
     "status": "ok",
     "timestamp": 1589194177352,
     "user": {
      "displayName": "Srikaran Elakurthy",
      "photoUrl": "",
      "userId": "11196471557604689597"
     },
     "user_tz": 240
    },
    "id": "I3K2HRQskyKP",
    "outputId": "8a83cea8-2cb4-4cd3-dfd8-3d2c34314cb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf\n",
      "Normalizer\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=\"english\",ngram_range=(1,2))\n",
    "x_train1 = tfidf.fit_transform(lyrics)\n",
    "x_test1 = tfidf.transform(tlyrics)\n",
    "print(\"tfidf\")\n",
    "posx_train=posdf.copy()\n",
    "posx_test=tposdf.copy()\n",
    "scaler = Normalizer().fit(posx_train)\n",
    "normalizedX_tr = scaler.transform(posx_train)\n",
    "normalizedX_ts = scaler.transform(posx_test)\n",
    "x_train=hstack( [x_train1,normalizedX_tr] )\n",
    "x_test=hstack( [x_test1,normalizedX_ts] )\n",
    "print(\"Normalizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-g2yiqaELTfu"
   },
   "source": [
    "Performing Multi nomial Naive Bayes model with smoothing allowed and storing the model into a pickle file and results with actual and predicted into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0Lv9-Lqwq4W"
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB(alpha=1.0)\n",
    "clf.fit(x_train, genre)\n",
    "y_pred = clf.predict(x_test)\n",
    "joblib.dump(clf, 'Naiveposrockred_full.pkl')\n",
    "score = accuracy_score(tgenre, y_pred)\n",
    "nairep=classification_report(tgenre, y_pred, target_names=df.genre.unique())\n",
    "dat={'Actual':tgenre,'pred':y_pred}\n",
    "resdf=pd.DataFrame(dat)\n",
    "resdf.to_csv('respos_naiverockred_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SycLkdAQLoyJ"
   },
   "source": [
    "Computing the classification report and store into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cIOgnBNJkyKT"
   },
   "outputs": [],
   "source": [
    "f=open(\"Naivepos_results_full.txt\",\"a\")\n",
    "\n",
    "f.write(\"\\n report:\\n\")\n",
    "dfr=pd.read_csv(\"respos_naiverockred_full.csv\")\n",
    "dfr.drop(dfr.columns[dfr.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "f.write(classification_report(dfr.Actual, dfr.pred, target_names=df.genre.unique()))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UAb2j1rdLxpC"
   },
   "source": [
    "Declaring the logistic model with multinomial as parameter specifying that we are trying to solve a multi classification problem and sag optimiazation is used as it is best for large datasets and have faster convergence.\n",
    "\n",
    "\n",
    "Fitting the model and storing into a pickle file\n",
    "\n",
    "predicting the results and storing into a csv file consisting the actual vs predicted for test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zeV_5-x7kyKV"
   },
   "outputs": [],
   "source": [
    "logi =  LogisticRegression(multi_class='multinomial',solver='sag')\n",
    "logi.fit(x_train, genre)\n",
    "y_pred = logi.predict(x_test)\n",
    "joblib.dump(logi, 'logiposrockred_full.pkl')\n",
    "score = accuracy_score(tgenre, y_pred)\n",
    "logrep=classification_report(tgenre, y_pred, target_names=df.genre.unique())\n",
    "dat={'Actual':tgenre,'pred':y_pred}\n",
    "resdf=pd.DataFrame(dat)\n",
    "resdf.to_csv('respos_logirockred_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QtNXxHJoMNKg"
   },
   "source": [
    "Computing the classification report and store into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGRVmF6rkyKW"
   },
   "outputs": [],
   "source": [
    "f=open(\"logipos_results_full.txt\",\"a\")\n",
    "f.write(\"\\n report:\\n\")\n",
    "dfr=pd.read_csv(\"respos_logirockred_full.csv\")\n",
    "dfr.drop(dfr.columns[dfr.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "f.write(classification_report(dfr.Actual, dfr.pred, target_names=df.genre.unique()))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iEeyhO6FkyKi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rYMIZndWMNvU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88U7uSEtkyKj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Naive_Bayesandlogi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
