{"nbformat":4,"nbformat_minor":0,"metadata":{"environment":{"name":"tf2-gpu.2-1.m47","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m47"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"DataProcessing.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"bHC0ZyxoXED8","colab_type":"text"},"source":["#Author: Aishwarya Varala\n","\n","#Description\n","\n","\n","\n","> The dataset is a musical lyrics dataset obtained from kaggle. In this file the lyrics will be extracted and cleaned. The cleaning of the dataset is as follows:\n","\n","\n","*   Firstly, the rows not having a genre are removed as it is our target variable.\n","*   Detecting the non-english lyrics and removing them by using the 'lang_detect package'.\n","*   Removing any emoji's present in text and also removing the numbers and special characters except the '!' symbol.\n","*   For Stemming, Porter Stemmer is being used.\n","\n","The resultant cleaned and preprocessed dataset is stored in a csv file namely processed_lyics.csv\n","\n","\n","> Oversampling is performed on the resultant dataset by decreasing the Rock genre by 50% and increasing all other class genres to match up with the number of Rock Genre records.\n"," \n","After oversampling the dataset is shuffled for a couple of times to avoid any continuous sequenes of lyrics belonging to the same genre. \n","\n","The resultant oversampled Dataset is stored in cleaneddata_oversampled.csv.\n","\n","\n","# Command to run the file\n","\n","\n","> Open the ipynb notebook in Jupyter Lab and go to the menu bar on the top, click on 'Run' and from the dropdown select the 'Run All' option to run all the cells in the notebook. \n","\n","\n","# Inputs and Outputs\n","\n","> Input: The input to the file is lyrics.csv which is the main dataset that contains all the relevant attributes. It is already read through the read_csv() function in the code. \n","\n","> Output: The processed_lyics.csv is generated as the output which is a representation of pre-processed data and no sampling. The cleaneddata_oversampled.csv file is also an output which is the distribution of oversampled instances of other genres and undersampled instances of the Rock genre.\n","\n","\n","\n","*The inputs to the program must be in the same folder as the script.\n","\n","INPUT: The lyrics file with each lyric as a row\n","OUTPUT: Pre-processed lyrics of each lyric in the input file\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"kPZhtYw-jeMy","colab_type":"code","outputId":"c23e7015-5f40-4fb6-8e86-3f1b3c230fc5","executionInfo":{"status":"ok","timestamp":1589247885285,"user_tz":240,"elapsed":20073,"user":{"displayName":"Srikaran Elakurthy","photoUrl":"","userId":"11196471557604689597"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0UiyyISHW689","colab_type":"code","outputId":"a91fcf73-6727-4908-ad5d-3ac805a74a76","executionInfo":{"status":"ok","timestamp":1589247709412,"user_tz":240,"elapsed":368,"user":{"displayName":"Srikaran Elakurthy","photoUrl":"","userId":"11196471557604689597"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["\n","#Importing the required python packages for preprocessing \n","import numpy as np\n","import pandas as pd\n","import re\n","from nltk import pos_tag\n","import nltk\n","from sklearn.utils import shuffle\n","\n","nltk.download('averaged_perceptron_tagger')\n","from langdetect import detect\n","import nltk\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.stem import PorterStemmer\n","import emoji\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.naive_bayes import MultinomialNB\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import classification_report\n","\n","# Removing the emoji from the text. Accepts a single text record and outputs the cleaned text data free of emoji's.\n","def text_has_emoji(text):\n","    i=0\n","    new_str=''\n","    arr=[]\n","    for character in text:\n","        if character in emoji.UNICODE_EMOJI:\n","             arr.append(character)\n","        else:\n","            new_str = new_str + text[i]\n","        i=i+1\n","    return new_str\n","\n","# Accepts a text record and calls the text_has_emoji function for freeing the text data from emoji's and tokenizes the text and lowercase it at same time. \n","#Outputs a list of tokens extracted from the text. Tokens are extracted using the Tokenizer library \n","#and with our custom made regular expression specifying removing numbers and cleaning out of symbols except '!'.\n","def tokenextractor(text):\n","    new_text=text_has_emoji(text)\n","    pattern = '[0-9]'\n","    t=re.sub(pattern, '', new_text)\n","    tokenizer = RegexpTokenizer('\\w+|\\!')\n","    tokens=tokenizer.tokenize(t.lower())\n","    return tokens\n","#Accepts a record in dataframe and derives the lyrics and calls the tokenextractor then the tokens will be derived from it.The resultant tokens are stemmed using \n","#Porter Stemmer and joined them. the resultant output will be the text which has been stemmed and clenaed .\n","def tokenize_and_stem(tok_df):\n","    #print(sentence_set)\n","    #return_set = []\n","    stemmed_tokens=[]\n","    sentence_set=tok_df['lyrics']\n","    tokens = tokenextractor(sentence_set)\n","    stems = []\n","    ps=PorterStemmer()\n","    for j in tokens:\n","        stems.append(ps.stem(j))\n","        stemmed_tokens.append(ps.stem(j))\n","        return_set=' '.join(stems)\n","    return(return_set)\n","# Accepts a record in a dataframe and derives lyrics and determines whether it is a english lyrics or not. We will determine this using detect function which will \n","#give us the language of the text. Then we will match whether the choosen text is eng or not. If it is eng we will return en_df =1 pr else 2.\n","def detect_lyric(en_df):\n","    en_flg=0\n","    #print(en_df)\n","    lyrics_txt=en_df['lyrics']\n","    pattern = r'(?=[a-z])'\n","    pattern = re.compile(pattern, re.IGNORECASE)\n","    matches = re.search(pattern, lyrics_txt)\n","    #print(lyrics_txt)\n","    if(bool(matches)):\n","        lan=detect(str(lyrics_txt))\n","        if(lan=='en'):\n","            en_df['is_eng']=1\n","    else:\n","        en_df['is_eng']=2\n","    return en_df\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uutLx3aDeDS2","colab_type":"text"},"source":["Importing the Lyrics dataset"]},{"cell_type":"code","metadata":{"id":"1bIVul8qW0to","colab_type":"code","colab":{}},"source":["music_df=pd.read_csv(\"lyrics.csv\")\n","music_df=music_df.dropna(how='any',axis=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3GoWMb_keHhW","colab_type":"text"},"source":["Checking missing values and removinng them.\n","Checking the language of lyrics and removing any non-english lyrics."]},{"cell_type":"code","metadata":{"id":"vFvocXUHW0ts","colab_type":"code","colab":{}},"source":["missing_music_df = music_df[music_df.genre == 'Not Available']\n","music_df=music_df[music_df.genre != 'Not Available']\n","music_df=music_df.apply(detect_lyric,axis=1)\n","music_df = music_df[music_df.is_eng == 1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2eU89NCTeUdN","colab_type":"text"},"source":["Checking the missing values in all columns "]},{"cell_type":"code","metadata":{"id":"mqbZlnizW0tw","colab_type":"code","outputId":"6a855b2d-4408-462c-d121-f63d6832d320","colab":{}},"source":["music_df.isnull().sum()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["artist    0\n","genre     0\n","index     0\n","is_eng    0\n","lyrics    0\n","song      0\n","year      0\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"wWqKV_2nW0tz","colab_type":"code","outputId":"2833ec2f-d51a-47a7-91bf-a864bebc69cc","colab":{}},"source":["music_df.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(220650, 7)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"n7wa-nnQebMf","colab_type":"text"},"source":["transforming the raw text data to get cleaned and transformed the text data which is free of numbers, Special symbols(except '!' ) and lowercase the whole text data and perform stemming on it."]},{"cell_type":"code","metadata":{"id":"JsnBcN07W0t2","colab_type":"code","colab":{}},"source":["music_df['lyrics']=music_df[[\"lyrics\"]].apply(tokenize_and_stem,axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fRjQmHBle2-5","colab_type":"text"},"source":["Exporting the cleaned and processed data to processed_lyics.csv file"]},{"cell_type":"code","metadata":{"id":"CRouf0PafHV8","colab_type":"code","colab":{}},"source":["music_df.to_csv('processed_lyics.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V14O1l8mfPG0","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"t9ctEhyMW0t3","colab_type":"code","colab":{}},"source":["df=pd.read_csv('processed_lyics.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J4aRcPaCW0t6","colab_type":"code","outputId":"0b5ca40a-d957-42f6-e162-aff3b9d7e113","colab":{}},"source":["df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>lyrics</th>\n","      <th>genre</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>173586</td>\n","      <td>i wait for the pain it alway come again and i ...</td>\n","      <td>Rock</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>192873</td>\n","      <td>as i hear the mock bird i rememb the word when...</td>\n","      <td>Country</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>196702</td>\n","      <td>gab yeah yeah x lateef blackalici lateef the t...</td>\n","      <td>Hip-Hop</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>34320</td>\n","      <td>well the sky broke in two i found you danc alo...</td>\n","      <td>Pop</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>77537</td>\n","      <td>when madam pompadour wa on a ballroom floor sa...</td>\n","      <td>Jazz</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0                                             lyrics    genre\n","0      173586  i wait for the pain it alway come again and i ...     Rock\n","1      192873  as i hear the mock bird i rememb the word when...  Country\n","2      196702  gab yeah yeah x lateef blackalici lateef the t...  Hip-Hop\n","3       34320  well the sky broke in two i found you danc alo...      Pop\n","4       77537  when madam pompadour wa on a ballroom floor sa...     Jazz"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"H5yPwthHf7QI","colab_type":"text"},"source":["Dropping the unnamed column which has been created when we are importing the library."]},{"cell_type":"code","metadata":{"id":"MH_KhLZjW0t8","colab_type":"code","colab":{}},"source":["df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z5XClPnkW0t-","colab_type":"code","outputId":"2e8a3f4d-0a44-491b-c157-4be6d720fd8c","colab":{}},"source":["df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>lyrics</th>\n","      <th>genre</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>i wait for the pain it alway come again and i ...</td>\n","      <td>Rock</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>as i hear the mock bird i rememb the word when...</td>\n","      <td>Country</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>gab yeah yeah x lateef blackalici lateef the t...</td>\n","      <td>Hip-Hop</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>well the sky broke in two i found you danc alo...</td>\n","      <td>Pop</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>when madam pompadour wa on a ballroom floor sa...</td>\n","      <td>Jazz</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              lyrics    genre\n","0  i wait for the pain it alway come again and i ...     Rock\n","1  as i hear the mock bird i rememb the word when...  Country\n","2  gab yeah yeah x lateef blackalici lateef the t...  Hip-Hop\n","3  well the sky broke in two i found you danc alo...      Pop\n","4  when madam pompadour wa on a ballroom floor sa...     Jazz"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"pDqGTMy_W0t_","colab_type":"code","outputId":"5d2cf762-ef35-4323-9d0c-264d3add5f64","colab":{}},"source":["df.isnull().sum()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["lyrics    0\n","genre     0\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"yPYJ3-g0W0uC","colab_type":"code","colab":{}},"source":["df_concat=df.copy()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7r0Gdjy-gYm1","colab_type":"text"},"source":["Over sampling the data. reducing the rock genre to 50% and increasing all other genre's respectively to match up with the rock.\n","\n","We will be preparing each genre dataframe and duplicate to a certain 'n' number of times so that it will match up with the Rock genre."]},{"cell_type":"code","metadata":{"id":"5obTnMP0W0uI","colab_type":"code","colab":{}},"source":["music_df_cou=df_concat[df_concat['genre']=='Country']\n","music_df_country= pd.concat([music_df_cou]*2, ignore_index=True)\n","#music_df_country.drop([\"index\"], axis = 1, inplace = True)\n","\n","music_df_Electron=df_concat[df_concat['genre']=='Electronic']\n","music_df_Electronic= pd.concat([music_df_Electron]*4, ignore_index=True)\n","#music_df_Electronic.drop([\"index\"], axis = 1, inplace = True)\n","\n","music_df_Folk=df_concat[df_concat['genre']=='Folk']\n","music_df_Fo= pd.concat([music_df_Folk]*16, ignore_index=True)\n","#music_df_Fo.drop([\"index\"], axis = 1, inplace = True)\n","\n","music_df_Hip=df_concat[df_concat['genre']=='Hip-Hop']\n","samp_hip=df_concat[df_concat['genre']=='Hip-Hop'].sample(n=8000)\n","music_df_Hiphop= pd.concat([music_df_Hip,samp_hip], ignore_index=True)\n","#music_df_Hiphop.drop([\"index\"], axis = 1, inplace = True)\n","\n","music_df_Ind=df_concat[df_concat['genre']=='Indie']\n","music_df_Indie= pd.concat([music_df_Ind]*9, ignore_index=True)\n","#music_df_Indie.drop([\"index\"], axis = 1, inplace = True)\n","\n","music_df_Ja=df_concat[df_concat['genre']=='Jazz']\n","music_df_Jazz= pd.concat([music_df_Ja]*3, ignore_index=True)\n","#music_df_Jazz.drop([\"index\"], axis = 1, inplace = True)\n","\n","music_df_Met=df_concat[df_concat['genre']=='Metal']\n","samp_met=df_concat[df_concat['genre']=='Metal'].sample(n=9000)\n","music_df_Metal= pd.concat([music_df_Met,samp_met], ignore_index=True)\n","#music_df_Metal.drop([\"index\"], axis = 1, inplace = True)\n","\n","music_df_oth=df_concat[df_concat['genre']=='Other']\n","music_df_other= pd.concat([music_df_oth]*7, ignore_index=True)\n","#music_df_other.drop([\"index\"], axis = 1, inplace = True)\n","\n","music_df_Pop=df_concat[df_concat['genre']=='Pop']\n","\n","music_df_r=df_concat[df_concat['genre']=='R&B']\n","music_df_rb= pd.concat([music_df_r]*8, ignore_index=True)\n","#music_df_rb.drop([\"index\"], axis = 1, inplace = True)\n","music_df_rock=df_concat[df_concat['genre']=='Rock'].sample(frac =.5)\n","#music_df_rock.drop([\"index\"], axis = 1, inplace = True)\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Po-Vb839g6Ln","colab_type":"text"},"source":["We will concatenate all the individual genre dataframes into one dataframe"]},{"cell_type":"code","metadata":{"id":"hHSYtHomW0uJ","colab_type":"code","colab":{}},"source":["processed_df=pd.concat([music_df_rb,music_df_Pop,music_df_other,music_df_Metal,music_df_Jazz,music_df_Indie,music_df_Hiphop,music_df_Fo,music_df_Electronic,music_df_country,music_df_rock],ignore_index=True)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yK23-2ZCiSXu","colab_type":"text"},"source":["From the above step the resultant concatenated dataframe will have a series of stagnated same genre's. So we will shuffle the data by using shuffle function from sklearn.utils.\n","\n","We will run the below cell multiple times so that a efficient shuffled dataset is formed."]},{"cell_type":"code","metadata":{"id":"8MCeHb3-W0uP","colab_type":"code","colab":{}},"source":["processed_df=shuffle(processed_df)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TMg6jQ8_ixpG","colab_type":"text"},"source":["Exporting the cleaned oversampled data to a csv file named:\n","cleaneddata_oversampled.csv"]},{"cell_type":"code","metadata":{"id":"E6O_etk_W0uQ","colab_type":"code","colab":{}},"source":["processed_df.to_csv(\"cleaneddata_oversampled.csv\")"],"execution_count":0,"outputs":[]}]}